{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slp989CftnxI"
      },
      "source": [
        "# Part 2: Streaming application using Spark Structured Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGiiCNattnxu"
      },
      "source": [
        "1. Write code to create a SparkSession with the following requirements: 1) use four cores with a proper application name; 2) Melbourne timezone; 3) a checkpoint location has been set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59wWkvgwtnxz"
      },
      "outputs": [],
      "source": [
        "## Initiate spark session\n",
        "\n",
        "# Import necessary packages for the integration between spark and kafka\n",
        "import os\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Setup Spark configuration\n",
        "master = \"local[4]\"\n",
        "app_name = \"Assignment 2B\"\n",
        "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
        "\n",
        "# Create a SparkSession using Melbourne time as the session timezone\n",
        "# Set up checkpoint location\n",
        "spark = SparkSession.builder.config(conf=spark_conf) \\\n",
        "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\") \\\n",
        "    .config(\"spark.checkpoint.dir\", \"parquet\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a SparkContext object using SparkSession\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Set log level to ERROR\n",
        "sc.setLogLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugOu_563tnx8"
      },
      "outputs": [],
      "source": [
        "## Import module\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import LongType\n",
        "from pyspark.sql.types import DateType\n",
        "from pyspark.sql.types import ArrayType\n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import TimestampType\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import round as round_\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import sum as sum_\n",
        "from pyspark.sql.functions import window\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "\n",
        "from pyspark.ml.classification import GBTClassificationModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBJU8b_4tnx9"
      },
      "source": [
        "2. Similar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets(previous_application_static and value_dict) into data frames. (You can reuse your code from 2A.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jxJqjmItnx-",
        "outputId": "25d458fd-bcb0-4182-d612-4773d40d5843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id_app: integer (nullable = true)\n",
            " |-- contract_type: integer (nullable = true)\n",
            " |-- amt_annuity: float (nullable = true)\n",
            " |-- amt_application: float (nullable = true)\n",
            " |-- amt_credit: float (nullable = true)\n",
            " |-- amt_down_payment: float (nullable = true)\n",
            " |-- amt_goods_price: float (nullable = true)\n",
            " |-- hour_appr_process_start: integer (nullable = true)\n",
            " |-- rate_down_payment: float (nullable = true)\n",
            " |-- rate_interest_primary: float (nullable = true)\n",
            " |-- rate_interest_privileged: float (nullable = true)\n",
            " |-- name_cash_loan_purpose: string (nullable = true)\n",
            " |-- name_contract_status: string (nullable = true)\n",
            " |-- days_decision: float (nullable = true)\n",
            " |-- name_payment_type: string (nullable = true)\n",
            " |-- code_rejection_reason: string (nullable = true)\n",
            " |-- name_type_suite: string (nullable = true)\n",
            " |-- name_client_type: string (nullable = true)\n",
            " |-- name_goods_category: string (nullable = true)\n",
            " |-- name_portfolio: string (nullable = true)\n",
            " |-- name_product_type: string (nullable = true)\n",
            " |-- channel_type: string (nullable = true)\n",
            " |-- sellerplace_area: integer (nullable = true)\n",
            " |-- name_seller_industry: string (nullable = true)\n",
            " |-- cnt_payment: float (nullable = true)\n",
            " |-- name_yield_group: string (nullable = true)\n",
            " |-- product_combination: string (nullable = true)\n",
            " |-- days_first_drawing: float (nullable = true)\n",
            " |-- days_first_due: float (nullable = true)\n",
            " |-- days_last_due_1st_version: float (nullable = true)\n",
            " |-- days_last_due: float (nullable = true)\n",
            " |-- days_termination: float (nullable = true)\n",
            " |-- nflag_insured_on_approval: float (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Load previous_application_static into dataframe\n",
        "## Reuse code from assignment 2A\n",
        "\n",
        "# Define schemas for previous_application following metadata file\n",
        "columns_to_cast_previous_application = {\n",
        "    \"id\": LongType(),\n",
        "    \"id_app\": IntegerType(),\n",
        "    \"contract_type\": IntegerType(),\n",
        "    \"hour_appr_process_start\": IntegerType(),\n",
        "    \"channel_type\": StringType(),\n",
        "    \"nflag_insured_on_approval\": FloatType(),\n",
        "    \"days_decision\": FloatType(),\n",
        "    \"days_first_drawing\": FloatType(),\n",
        "    \"days_first_due\": FloatType(),\n",
        "    \"days_last_due_1st_version\": FloatType(),\n",
        "    \"days_last_due\": FloatType(),\n",
        "    \"days_termination\": FloatType(),\n",
        "    \"rate_down_payment\": FloatType(),\n",
        "    \"rate_interest_primary\": FloatType(),\n",
        "    \"rate_interest_privileged\": FloatType(),\n",
        "    \"amt_annuity\": FloatType(),\n",
        "    \"amt_application\": FloatType(),\n",
        "    \"amt_credit\": FloatType(),\n",
        "    \"amt_down_payment\": FloatType(),\n",
        "    \"amt_goods_price\": FloatType(),\n",
        "    \"name_cash_loan_purpose\": StringType(),\n",
        "    \"name_contract_status\": StringType(),\n",
        "    \"name_payment_type\": StringType(),\n",
        "    \"code_rejection_reason\": StringType(),\n",
        "    \"name_type_suite\": StringType(),\n",
        "    \"name_client_type\": StringType(),\n",
        "    \"name_goods_category\": StringType(),\n",
        "    \"name_portfolio\": StringType(),\n",
        "    \"name_product_type\": StringType(),\n",
        "    \"sellerplace_area\": IntegerType(),\n",
        "    \"name_seller_industry\": StringType(),\n",
        "    \"cnt_payment\": FloatType(),\n",
        "    \"name_yield_group\": StringType(),\n",
        "    \"product_combination\": StringType()\n",
        "}\n",
        "\n",
        "# Load the previous_application file into dataframe\n",
        "df_previous_application = spark.read.csv(\"previous_application_static.csv\",header=True)\n",
        "\n",
        "# Cast schema datatype to the dataframe\n",
        "for column, data_type in columns_to_cast_previous_application.items():\n",
        "    df_previous_application = df_previous_application.withColumn(column, col(column).cast(data_type))\n",
        "\n",
        "# Print schema\n",
        "df_previous_application.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbwMkBO5tnyA",
        "outputId": "832db348-efc3-47c9-dd66-6fc556ccb8f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- key: string (nullable = true)\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Load value_dict into dataframe\n",
        "## Reuse code from assignment 2A\n",
        "\n",
        "df_value_dict = spark.read.csv(\"value_dict.csv\",header=True)\n",
        "\n",
        "# Print schema\n",
        "df_value_dict.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg7VZyeztnyI"
      },
      "source": [
        "3. Using the Kafka topic from the producer in Task 1, read the streaming data with Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0Li1lAUtnyJ"
      },
      "outputs": [],
      "source": [
        "## Connect to Kafka Producer, subscribe to the topic and load data from Kafka topic with readStream\n",
        "\n",
        "# Configuration\n",
        "hostip = \"kafka\"\n",
        "topic = \"application_stream\"\n",
        "\n",
        "df = (spark\n",
        "      .readStream\n",
        "      .format(\"kafka\")\n",
        "      .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
        "      .option(\"subscribe\", topic)\n",
        "      .load()\n",
        "     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY5bbXUXtnyL"
      },
      "outputs": [],
      "source": [
        "# Convert the key value pair from kafka stream to string\n",
        "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaygL0xwtnyL"
      },
      "outputs": [],
      "source": [
        "# Define the schema for the structured datastream received\n",
        "schema = ArrayType(StructType([\n",
        "    StructField(\"id_app\", StringType(), True),\n",
        "    StructField(\"target\", StringType(), True),\n",
        "    StructField(\"contract_type\", StringType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"own_car\", StringType(), True),\n",
        "    StructField(\"own_property\", StringType(), True),\n",
        "    StructField(\"num_of_children\", StringType(), True),\n",
        "    StructField(\"income_total\", StringType(), True),\n",
        "    StructField(\"amt_credit\", StringType(), True),\n",
        "    StructField(\"amt_annuity\", StringType(), True),\n",
        "    StructField(\"amt_goods_price\", StringType(), True),\n",
        "    StructField(\"income_type\", StringType(), True),\n",
        "    StructField(\"education_type\", StringType(), True),\n",
        "    StructField(\"family_status\", StringType(), True),\n",
        "    StructField(\"housing_type\", StringType(), True),\n",
        "    StructField(\"region_population\", StringType(), True),\n",
        "    StructField(\"days_birth\", StringType(), True),\n",
        "    StructField(\"days_employed\", StringType(), True),\n",
        "    StructField(\"own_car_age\", StringType(), True),\n",
        "    StructField(\"flag_mobile\", StringType(), True),\n",
        "    StructField(\"flag_emp_phone\", StringType(), True),\n",
        "    StructField(\"flag_work_phone\", StringType(), True),\n",
        "    StructField(\"flag_cont_mobile\", StringType(), True),\n",
        "    StructField(\"flag_phone\", StringType(), True),\n",
        "    StructField(\"flag_email\", StringType(), True),\n",
        "    StructField(\"occupation_type\", StringType(), True),\n",
        "    StructField(\"cnt_fam_members\", StringType(), True),\n",
        "    StructField(\"weekday_app_process_start\", StringType(), True),\n",
        "    StructField(\"hour_app_process_start\", StringType(), True),\n",
        "    StructField(\"organization_type\", StringType(), True),\n",
        "    StructField(\"credit_score_1\", StringType(), True),\n",
        "    StructField(\"credit_score_2\", StringType(), True),\n",
        "    StructField(\"credit_score_3\", StringType(), True),\n",
        "    StructField(\"days_last_phone_change\", StringType(), True),\n",
        "    StructField(\"amt_credit_req_last_hour\", StringType(), True),\n",
        "    StructField(\"amt_credit_req_last_day\", StringType(), True),\n",
        "    StructField(\"amt_credit_req_last_week\", StringType(), True),\n",
        "    StructField(\"amt_credit_req_last_month\", StringType(), True),\n",
        "    StructField(\"amt_credit_req_last_quarter\", StringType(), True),\n",
        "    StructField(\"amt_credit_req_last_year\", StringType(), True),\n",
        "    StructField(\"ts\", IntegerType(), True)\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgFes8OytnyQ"
      },
      "outputs": [],
      "source": [
        "# Use from_json to parse the string to the json format based on the defined schema\n",
        "df = df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('parsed_value'))\n",
        "df = df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNUqWxk2tnyQ"
      },
      "outputs": [],
      "source": [
        "# Rename the column with proper name\n",
        "df_formatted = df.select(\n",
        "                    F.col(\"unnested_value.id_app\").alias(\"id_app\"),\n",
        "                    F.col(\"unnested_value.target\").alias(\"target\"),\n",
        "                    F.col(\"unnested_value.contract_type\").alias(\"contract_type\"),\n",
        "                    F.col(\"unnested_value.gender\").alias(\"gender\"),\n",
        "                    F.col(\"unnested_value.own_car\").alias(\"own_car\"),\n",
        "                    F.col(\"unnested_value.own_property\").alias(\"own_property\"),\n",
        "                    F.col(\"unnested_value.num_of_children\").alias(\"num_of_children\"),\n",
        "                    F.col(\"unnested_value.income_total\").alias(\"income_total\"),\n",
        "                    F.col(\"unnested_value.amt_credit\").alias(\"amt_credit\"),\n",
        "                    F.col(\"unnested_value.amt_annuity\").alias(\"amt_annuity\"),\n",
        "                    F.col(\"unnested_value.amt_goods_price\").alias(\"amt_goods_price\"),\n",
        "                    F.col(\"unnested_value.income_type\").alias(\"income_type\"),\n",
        "                    F.col(\"unnested_value.education_type\").alias(\"education_type\"),\n",
        "                    F.col(\"unnested_value.family_status\").alias(\"family_status\"),\n",
        "                    F.col(\"unnested_value.housing_type\").alias(\"housing_type\"),\n",
        "                    F.col(\"unnested_value.region_population\").alias(\"region_population\"),\n",
        "                    F.col(\"unnested_value.days_birth\").alias(\"days_birth\"),\n",
        "                    F.col(\"unnested_value.days_employed\").alias(\"days_employed\"),\n",
        "                    F.col(\"unnested_value.own_car_age\").alias(\"own_car_age\"),\n",
        "                    F.col(\"unnested_value.flag_mobile\").alias(\"flag_mobile\"),\n",
        "                    F.col(\"unnested_value.flag_emp_phone\").alias(\"flag_emp_phone\"),\n",
        "                    F.col(\"unnested_value.flag_work_phone\").alias(\"flag_work_phone\"),\n",
        "                    F.col(\"unnested_value.flag_cont_mobile\").alias(\"flag_cont_mobile\"),\n",
        "                    F.col(\"unnested_value.flag_phone\").alias(\"flag_phone\"),\n",
        "                    F.col(\"unnested_value.flag_email\").alias(\"flag_email\"),\n",
        "                    F.col(\"unnested_value.occupation_type\").alias(\"occupation_type\"),\n",
        "                    F.col(\"unnested_value.cnt_fam_members\").alias(\"cnt_fam_members\"),\n",
        "                    F.col(\"unnested_value.weekday_app_process_start\").alias(\"weekday_app_process_start\"),\n",
        "                    F.col(\"unnested_value.hour_app_process_start\").alias(\"hour_app_process_start\"),\n",
        "                    F.col(\"unnested_value.organization_type\").alias(\"organization_type\"),\n",
        "                    F.col(\"unnested_value.credit_score_1\").alias(\"credit_score_1\"),\n",
        "                    F.col(\"unnested_value.credit_score_2\").alias(\"credit_score_2\"),\n",
        "                    F.col(\"unnested_value.credit_score_3\").alias(\"credit_score_3\"),\n",
        "                    F.col(\"unnested_value.days_last_phone_change\").alias(\"days_last_phone_change\"),\n",
        "                    F.col(\"unnested_value.amt_credit_req_last_hour\").alias(\"amt_credit_req_last_hour\"),\n",
        "                    F.col(\"unnested_value.amt_credit_req_last_day\").alias(\"amt_credit_req_last_day\"),\n",
        "                    F.col(\"unnested_value.amt_credit_req_last_week\").alias(\"amt_credit_req_last_week\"),\n",
        "                    F.col(\"unnested_value.amt_credit_req_last_month\").alias(\"amt_credit_req_last_month\"),\n",
        "                    F.col(\"unnested_value.amt_credit_req_last_quarter\").alias(\"amt_credit_req_last_quarter\"),\n",
        "                    F.col(\"unnested_value.amt_credit_req_last_year\").alias(\"amt_credit_req_last_year\"),\n",
        "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q715SvnZtnyR",
        "outputId": "d476bb1a-e792-4ccb-b7bd-e5de59492fc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id_app: string (nullable = true)\n",
            " |-- target: string (nullable = true)\n",
            " |-- contract_type: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- own_car: string (nullable = true)\n",
            " |-- own_property: string (nullable = true)\n",
            " |-- num_of_children: string (nullable = true)\n",
            " |-- income_total: string (nullable = true)\n",
            " |-- amt_credit: string (nullable = true)\n",
            " |-- amt_annuity: string (nullable = true)\n",
            " |-- amt_goods_price: string (nullable = true)\n",
            " |-- income_type: string (nullable = true)\n",
            " |-- education_type: string (nullable = true)\n",
            " |-- family_status: string (nullable = true)\n",
            " |-- housing_type: string (nullable = true)\n",
            " |-- region_population: string (nullable = true)\n",
            " |-- days_birth: string (nullable = true)\n",
            " |-- days_employed: string (nullable = true)\n",
            " |-- own_car_age: string (nullable = true)\n",
            " |-- flag_mobile: string (nullable = true)\n",
            " |-- flag_emp_phone: string (nullable = true)\n",
            " |-- flag_work_phone: string (nullable = true)\n",
            " |-- flag_cont_mobile: string (nullable = true)\n",
            " |-- flag_phone: string (nullable = true)\n",
            " |-- flag_email: string (nullable = true)\n",
            " |-- occupation_type: string (nullable = true)\n",
            " |-- cnt_fam_members: string (nullable = true)\n",
            " |-- weekday_app_process_start: string (nullable = true)\n",
            " |-- hour_app_process_start: string (nullable = true)\n",
            " |-- organization_type: string (nullable = true)\n",
            " |-- credit_score_1: string (nullable = true)\n",
            " |-- credit_score_2: string (nullable = true)\n",
            " |-- credit_score_3: string (nullable = true)\n",
            " |-- days_last_phone_change: string (nullable = true)\n",
            " |-- amt_credit_req_last_hour: string (nullable = true)\n",
            " |-- amt_credit_req_last_day: string (nullable = true)\n",
            " |-- amt_credit_req_last_week: string (nullable = true)\n",
            " |-- amt_credit_req_last_month: string (nullable = true)\n",
            " |-- amt_credit_req_last_quarter: string (nullable = true)\n",
            " |-- amt_credit_req_last_year: string (nullable = true)\n",
            " |-- ts: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the schema to inspect the data type\n",
        "df_formatted.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xJmBY--tnyS"
      },
      "source": [
        "4. Then, transform the streaming data format into proper types following the metadata file schema, similar to assignment 2A. Perform the following tasks:  \n",
        "a) For the 'ts' column, convert it to the timestamp format, we will use it as event_time.  \n",
        "b) If the data is late for more than 1 minute, discard it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPEd9y8xtnyS",
        "outputId": "89bb4f82-a2c6-4b3b-89e1-753484d074a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id_app: integer (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            " |-- contract_type: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- own_car: string (nullable = true)\n",
            " |-- own_property: string (nullable = true)\n",
            " |-- num_of_children: integer (nullable = true)\n",
            " |-- income_total: float (nullable = true)\n",
            " |-- amt_credit: float (nullable = true)\n",
            " |-- amt_annuity: float (nullable = true)\n",
            " |-- amt_goods_price: float (nullable = true)\n",
            " |-- income_type: integer (nullable = true)\n",
            " |-- education_type: integer (nullable = true)\n",
            " |-- family_status: integer (nullable = true)\n",
            " |-- housing_type: integer (nullable = true)\n",
            " |-- region_population: float (nullable = true)\n",
            " |-- days_birth: integer (nullable = true)\n",
            " |-- days_employed: integer (nullable = true)\n",
            " |-- own_car_age: float (nullable = true)\n",
            " |-- flag_mobile: integer (nullable = true)\n",
            " |-- flag_emp_phone: integer (nullable = true)\n",
            " |-- flag_work_phone: integer (nullable = true)\n",
            " |-- flag_cont_mobile: integer (nullable = true)\n",
            " |-- flag_phone: integer (nullable = true)\n",
            " |-- flag_email: integer (nullable = true)\n",
            " |-- occupation_type: integer (nullable = true)\n",
            " |-- cnt_fam_members: float (nullable = true)\n",
            " |-- weekday_app_process_start: string (nullable = true)\n",
            " |-- hour_app_process_start: integer (nullable = true)\n",
            " |-- organization_type: integer (nullable = true)\n",
            " |-- credit_score_1: float (nullable = true)\n",
            " |-- credit_score_2: float (nullable = true)\n",
            " |-- credit_score_3: float (nullable = true)\n",
            " |-- days_last_phone_change: integer (nullable = true)\n",
            " |-- amt_credit_req_last_hour: float (nullable = true)\n",
            " |-- amt_credit_req_last_day: float (nullable = true)\n",
            " |-- amt_credit_req_last_week: float (nullable = true)\n",
            " |-- amt_credit_req_last_month: float (nullable = true)\n",
            " |-- amt_credit_req_last_quarter: float (nullable = true)\n",
            " |-- amt_credit_req_last_year: float (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Transform stream data format according to metadata file schema\n",
        "## Reuse code from assignment 2A\n",
        "\n",
        "# Define schemas for application_data following metadata file\n",
        "# Convert ts column to timestamp format\n",
        "columns_to_cast_application = {\n",
        "    \"id_app\": IntegerType(),\n",
        "    \"target\": IntegerType(),\n",
        "    \"contract_type\": IntegerType(),\n",
        "    \"gender\": StringType(),\n",
        "    \"own_car\": StringType(),\n",
        "    \"own_property\": StringType(),\n",
        "    \"num_of_children\": IntegerType(),\n",
        "    \"income_total\": FloatType(),\n",
        "    \"amt_credit\": FloatType(),\n",
        "    \"amt_annuity\": FloatType(),\n",
        "    \"amt_goods_price\": FloatType(),\n",
        "    \"income_type\": IntegerType(),\n",
        "    \"education_type\": IntegerType(),\n",
        "    \"family_status\": IntegerType(),\n",
        "    \"housing_type\": IntegerType(),\n",
        "    \"region_population\": FloatType(),\n",
        "    \"days_birth\": IntegerType(),\n",
        "    \"days_employed\": IntegerType(),\n",
        "    \"own_car_age\": FloatType(),\n",
        "    \"flag_mobile\": IntegerType(),\n",
        "    \"flag_emp_phone\": IntegerType(),\n",
        "    \"flag_work_phone\": IntegerType(),\n",
        "    \"flag_cont_mobile\": IntegerType(),\n",
        "    \"flag_phone\": IntegerType(),\n",
        "    \"flag_email\": IntegerType(),\n",
        "    \"occupation_type\": IntegerType(),\n",
        "    \"cnt_fam_members\": FloatType(),\n",
        "    \"weekday_app_process_start\": StringType(),\n",
        "    \"hour_app_process_start\": IntegerType(),\n",
        "    \"organization_type\": IntegerType(),\n",
        "    \"credit_score_1\": FloatType(),\n",
        "    \"credit_score_2\": FloatType(),\n",
        "    \"credit_score_3\": FloatType(),\n",
        "    \"days_last_phone_change\": IntegerType(),\n",
        "    \"amt_credit_req_last_hour\": FloatType(),\n",
        "    \"amt_credit_req_last_day\": FloatType(),\n",
        "    \"amt_credit_req_last_week\": FloatType(),\n",
        "    \"amt_credit_req_last_month\": FloatType(),\n",
        "    \"amt_credit_req_last_quarter\": FloatType(),\n",
        "    \"amt_credit_req_last_year\": FloatType(),\n",
        "    \"ts\": TimestampType()\n",
        "}\n",
        "\n",
        "# Cast schema datatype to the dataframe\n",
        "for column, data_type in columns_to_cast_application.items():\n",
        "    df_formatted = df_formatted.withColumn(column, col(column).cast(data_type))\n",
        "\n",
        "# Print schema\n",
        "df_formatted.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ib3Cj1jtnyT"
      },
      "outputs": [],
      "source": [
        "## Handling late data for more than 1 minute late\n",
        "\n",
        "df_final = df_formatted.withWatermark(\"ts\", \"1 minute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFwYFEkmtnyU"
      },
      "source": [
        "5. Join the static data frames with the streaming data frame, perform data type/column conversion according to your ML model and print out the Schema. (Again, you can reuse code from 2A).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVRrb5E-tnyU",
        "outputId": "64690178-cdd7-462b-995d-6aa0b6e6845e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- target: integer (nullable = true)\n",
            " |-- contract_type: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- own_property: string (nullable = true)\n",
            " |-- num_of_children: integer (nullable = true)\n",
            " |-- amt_credit: float (nullable = true)\n",
            " |-- amt_annuity: float (nullable = true)\n",
            " |-- amt_goods_price: float (nullable = true)\n",
            " |-- income_type: string (nullable = true)\n",
            " |-- education_type: string (nullable = true)\n",
            " |-- family_status: string (nullable = true)\n",
            " |-- housing_type: integer (nullable = true)\n",
            " |-- days_employed: integer (nullable = true)\n",
            " |-- own_car_age: float (nullable = true)\n",
            " |-- flag_cont_mobile: integer (nullable = true)\n",
            " |-- occupation_type: string (nullable = true)\n",
            " |-- cnt_fam_members: float (nullable = true)\n",
            " |-- organization_type: integer (nullable = true)\n",
            " |-- days_last_phone_change: integer (nullable = true)\n",
            " |-- amt_credit_req_last_year: float (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            " |-- loan_to_income_ratio: double (nullable = true)\n",
            " |-- age_bucket: string (nullable = true)\n",
            " |-- credit_bucket: string (nullable = true)\n",
            " |-- num_of_prev_app: long (nullable = true)\n",
            " |-- num_of_approved_app: long (nullable = true)\n",
            " |-- total_credit_to_income_ratio: double (nullable = true)\n",
            " |-- income_bucket: integer (nullable = true)\n",
            " |-- flag_contact: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Join and transform dataframe by repeating assignment 2A\n",
        "\n",
        "df_application = df_final.withColumn(\"loan_to_income_ratio\", round_(col(\"amt_credit\")/col(\"income_total\"), 4))\n",
        "\n",
        "## Reuse date_calculator and age_calculator function from assignment 1\n",
        "\n",
        "# This function takes a base date and a relative number to calculate the target date\n",
        "def date_calculator(base_date, relative_number):\n",
        "    base_date = str(base_date)\n",
        "\n",
        "    if relative_number is None:\n",
        "        return base_date\n",
        "    else:\n",
        "        relative_number = int(relative_number)\n",
        "        base_date_datetime = datetime.strptime(base_date, \"%d/%m/%Y\")\n",
        "        result_date = base_date_datetime + timedelta(days = relative_number)\n",
        "        return result_date.date()\n",
        "\n",
        "# This function takes birthdate and current_date to generate age\n",
        "def age_calculator(birthdate, current_date):\n",
        "        if birthdate is not None:\n",
        "            current_date = datetime.strptime(current_date, \"%d/%m/%Y\")\n",
        "            age = current_date.year - birthdate.year - ((current_date.month, current_date.day) < (birthdate.month, birthdate.day))\n",
        "            return age\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "# This function takes age and produce age bucket value\n",
        "def age_bucket(age):\n",
        "    age = int(age)\n",
        "    if age < 25:\n",
        "        age_bucket = \"Y\"\n",
        "    elif age < 35:\n",
        "        age_bucket = \"E\"\n",
        "    elif age < 45:\n",
        "        age_bucket = \"M\"\n",
        "    elif age < 55:\n",
        "        age_bucket = \"L\"\n",
        "    elif age < 65:\n",
        "        age_bucket = \"N\"\n",
        "    else:\n",
        "        age_bucket = \"R\"\n",
        "    return age_bucket\n",
        "\n",
        "## Calculate birthday\n",
        "current_date = datetime.today().strftime(\"%d/%m/%Y\")\n",
        "\n",
        "# Register date_calculator as udf\n",
        "date_calculator_udf = udf(date_calculator, DateType())\n",
        "\n",
        "# Generate birthdate\n",
        "df_application = df_application.withColumn(\"birthday\", date_calculator_udf(lit(current_date), col(\"days_birth\")))\n",
        "\n",
        "## Calculate age\n",
        "\n",
        "# Register age_calculator as udf\n",
        "age_calculator_udf = udf(age_calculator, IntegerType())\n",
        "\n",
        "# Generate age\n",
        "df_application = df_application.withColumn(\"age\", age_calculator_udf(col(\"birthday\"), lit(current_date)))\n",
        "\n",
        "## Calculate age_bucket\n",
        "\n",
        "# Register age_bucket as udf\n",
        "age_bucket_udf = udf(age_bucket, StringType())\n",
        "\n",
        "# Generate age bucket\n",
        "df_application = df_application.withColumn(\"age_bucket\", age_bucket_udf(col(\"age\")))\n",
        "\n",
        "# This function assigns credit worthiness bucket to avarage credit score\n",
        "def credit_bucket(credit_average):\n",
        "    credit_average = float(credit_average)\n",
        "    if credit_average < 0.4:\n",
        "        credit_bucket = \"low\"\n",
        "    elif credit_average < 0.7:\n",
        "        credit_bucket = \"medium\"\n",
        "    else:\n",
        "        credit_bucket = \"high\"\n",
        "\n",
        "    return credit_bucket\n",
        "\n",
        "## Reuse code from assignment 1\n",
        "# Replace null value in credit score columns\n",
        "df_application = df_application.withColumn(\"credit_score_1\", when(col(\"credit_score_1\").isNull(), 0.5).otherwise(col(\"credit_score_1\")))\n",
        "df_application = df_application.withColumn(\"credit_score_2\", when(col(\"credit_score_2\").isNull(), 0.5).otherwise(col(\"credit_score_2\")))\n",
        "df_application = df_application.withColumn(\"credit_score_3\", when(col(\"credit_score_3\").isNull(), 0.5).otherwise(col(\"credit_score_3\")))\n",
        "\n",
        "# Calculate average credit score\n",
        "df_application = (df_application\n",
        "                  .withColumn(\"average_credit_score\", (col(\"credit_score_1\") + col(\"credit_score_2\") + col(\"credit_score_3\"))/3)\n",
        "                  .withColumn(\"average_credit_score\", round_(col(\"average_credit_score\"), 4))\n",
        "                 )\n",
        "\n",
        "\n",
        "\n",
        "## Calculate credit_bucket\n",
        "\n",
        "# Register credit_bucket as udf\n",
        "credit_bucket_udf = udf(credit_bucket, StringType())\n",
        "\n",
        "# Generate age bucket\n",
        "df_application = df_application.withColumn(\"credit_bucket\", credit_bucket_udf(col(\"average_credit_score\")))\n",
        "\n",
        "# Get the number of previous applications\n",
        "df_num_of_prev_app = (df_previous_application\n",
        "                      .groupBy(\"id_app\")\n",
        "                      .count()\n",
        "                     )\n",
        "\n",
        "# Join the number of previous applications with application\n",
        "# For those without previous application, set the count to 0\n",
        "df_application = (df_application\n",
        "                  .join(df_num_of_prev_app, df_application.id_app == df_num_of_prev_app.id_app, \"left_outer\")\n",
        "                  .drop(df_num_of_prev_app[\"id_app\"])\n",
        "                  .withColumnRenamed(\"count\", \"num_of_prev_app\")\n",
        "                  .withColumn(\"num_of_prev_app\", when(col(\"num_of_prev_app\").isNull(), 0).otherwise(col(\"num_of_prev_app\")))\n",
        "                 )\n",
        "\n",
        "## num_of_approved_app (number of approved applications)\n",
        "\n",
        "# Get the number of approved applications\n",
        "df_num_of_approved_app = (df_previous_application\n",
        "                          .filter(col(\"name_contract_status\") == \"Approved\")\n",
        "                          .groupBy(\"id_app\")\n",
        "                          .count()\n",
        "                         )\n",
        "\n",
        "# Join the number of approved applications with application\n",
        "# For those without approved applications, set the count to 0\n",
        "df_application = (df_application\n",
        "                  .join(df_num_of_approved_app, df_application.id_app == df_num_of_approved_app.id_app, \"left_outer\")\n",
        "                  .drop(df_num_of_approved_app[\"id_app\"])\n",
        "                  .withColumnRenamed(\"count\", \"num_of_approved_app\")\n",
        "                  .withColumn(\"num_of_approved_app\", when(col(\"num_of_approved_app\").isNull(), 0).otherwise(col(\"num_of_approved_app\")))\n",
        "                 )\n",
        "\n",
        "## total_credit (sum of credit of all approved previous applications)\n",
        "\n",
        "# Get the sum of credit of all approved previous applications\n",
        "df_total_credit = (df_previous_application\n",
        "                   .filter(col(\"name_contract_status\") == \"Approved\")\n",
        "                   .groupBy(\"id_app\")\n",
        "                   .agg(sum_(\"amt_credit\").alias(\"total_credit\"))\n",
        "                   .withColumn(\"total_credit\", round_(col(\"total_credit\"), 1))\n",
        "                  )\n",
        "\n",
        "# Join the sum of credit of all approved previous applications with application\n",
        "# For those without approved previous applications, set the amount to 0\n",
        "df_application = (df_application\n",
        "                  .join(df_total_credit, df_application.id_app == df_total_credit.id_app, \"left_outer\")\n",
        "                  .drop(df_total_credit[\"id_app\"])\n",
        "                  .withColumn(\"total_credit\", when(col(\"total_credit\").isNull(), 0).otherwise(col(\"total_credit\")))\n",
        "                 )\n",
        "\n",
        "## total_credit_to_income_ratio (total credit/income)\n",
        "\n",
        "# Calculate total_credit_to_income_ratio\n",
        "df_application = (df_application\n",
        "                  .withColumn(\"total_credit_to_income_ratio\", col(\"total_credit\")/col(\"income_total\"))\n",
        "                  .withColumn(\"total_credit_to_income_ratio\", round_(col(\"total_credit_to_income_ratio\"), 2))\n",
        "                 )\n",
        "\n",
        "## education_type\n",
        "\n",
        "# Separate education_type key value pair from value dictionary\n",
        "df_education_type = (df_value_dict\n",
        "                     .filter(col(\"category\") == \"education_type\")\n",
        "                     .drop(col(\"id\"), col(\"category\"))\n",
        "                    )\n",
        "# Convert education_type dataframe to dictionary\n",
        "education_type_dict = {x[1]: x[0] for x in df_education_type.collect()}\n",
        "\n",
        "# Adjust education_type datatype, apply replace function based on dictionary\n",
        "df_application = (df_application\n",
        "                  .withColumn(\"education_type\", col(\"education_type\").cast(StringType()))\n",
        "                  .replace(education_type_dict, subset = [\"education_type\"])\n",
        "                 )\n",
        "\n",
        "## occupation_type\n",
        "\n",
        "# Separate occupation_type key value pair from value dictionary\n",
        "df_occupation_type = (df_value_dict\n",
        "                     .filter(col(\"category\") == \"occupation_type\")\n",
        "                     .drop(col(\"id\"), col(\"category\"))\n",
        "                    )\n",
        "# Convert occupation_type dataframe to dictionary\n",
        "occupation_type_dict = {x[1]: x[0] for x in df_occupation_type.collect()}\n",
        "\n",
        "# Adjust occupation_type datatype, apply replace function based on dictionary\n",
        "df_application = (df_application\n",
        "                  .withColumn(\"occupation_type\", col(\"occupation_type\").cast(StringType()))\n",
        "                  .replace(occupation_type_dict, subset = [\"occupation_type\"])\n",
        "                 )\n",
        "\n",
        "## income_type\n",
        "\n",
        "# Separate income_type key value pair from value dictionary\n",
        "df_income_type = (df_value_dict\n",
        "                     .filter(col(\"category\") == \"income_type\")\n",
        "                     .drop(col(\"id\"), col(\"category\"))\n",
        "                    )\n",
        "# Convert income_type dataframe to dictionary\n",
        "income_type_dict = {x[1]: x[0] for x in df_income_type.collect()}\n",
        "\n",
        "# Adjust income_type datatype, apply replace function based on dictionary\n",
        "df_application = (df_application\n",
        "                  .withColumn(\"income_type\", col(\"income_type\").cast(StringType()))\n",
        "                  .replace(income_type_dict, subset = [\"income_type\"])\n",
        "                 )\n",
        "\n",
        "## family_status\n",
        "\n",
        "# Separate family_status key value pair from value dictionary\n",
        "df_family_status = (df_value_dict\n",
        "                     .filter(col(\"category\") == \"family_status\")\n",
        "                     .drop(col(\"id\"), col(\"category\"))\n",
        "                    )\n",
        "# Convert family_status dataframe to dictionary\n",
        "family_status_dict = {x[1]: x[0] for x in df_family_status.collect()}\n",
        "\n",
        "# Adjust family_status datatype, apply replace function based on dictionary\n",
        "df_application = (df_application\n",
        "                  .withColumn(\"family_status\", col(\"family_status\").cast(StringType()))\n",
        "                  .replace(family_status_dict, subset = [\"family_status\"])\n",
        "                 )\n",
        "\n",
        "## Create income bucket\n",
        "\n",
        "# This function takes income and produce income bucket value\n",
        "def income_bucket(income):\n",
        "    if income is None:\n",
        "        income_bucket = 0\n",
        "    else:\n",
        "        income = int(income)\n",
        "        if income <= 18200:\n",
        "            income_bucket = 1\n",
        "        elif income <= 45000:\n",
        "            income_bucket = 2\n",
        "        elif income <= 120000:\n",
        "            income_bucket = 3\n",
        "        elif income <= 180000:\n",
        "            income_bucket = 4\n",
        "        else:\n",
        "            income_bucket = 5\n",
        "    return income_bucket\n",
        "\n",
        "\n",
        "# Register income_bucket as udf\n",
        "income_bucket_udf = udf(income_bucket, IntegerType())\n",
        "\n",
        "# Generate income bucket\n",
        "df_application = df_application.withColumn(\"income_bucket\", income_bucket_udf(col(\"income_total\")))\n",
        "\n",
        "## Creat flag_contact\n",
        "\n",
        "df_application = df_application.withColumn(\"flag_contact\", col(\"flag_mobile\") + col(\"flag_emp_phone\") + col(\"flag_work_phone\") + col(\"flag_phone\") + col(\"flag_email\"))\n",
        "\n",
        "## Drop columns\n",
        "\n",
        "df_application = df_application.drop(\"id_app\", \"age\", \"birthday\", \"days_birth\", \"average_credit_score\", \"credit_score_1\", \"credit_score_2\", \"credit_score_3\", \"income_total\", \"flag_mobile\", \"flag_emp_phone\", \"flag_work_phone\", \"flag_phone\", \"flag_email\", \"amt_credit_req_last_hour\", \"amt_credit_req_last_day\", \"amt_credit_req_last_week\", \"amt_credit_req_last_month\", \"amt_credit_req_last_quarter\", \"weekday_app_process_start\", \"hour_app_process_start\", \"region_population\", \"total_credit\", \"own_car\")\n",
        "\n",
        "# Display the schema\n",
        "df_application.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYhGCrI7tnyY"
      },
      "outputs": [],
      "source": [
        "# Create output sink for the stream to inspect records\n",
        "query = (df_final\n",
        "         .writeStream.outputMode(\"append\")\n",
        "         .format(\"console\")\n",
        "         .trigger(processingTime='5 seconds')\n",
        "         .start()\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjnxbuEktnyZ"
      },
      "outputs": [],
      "source": [
        "# Stop the query\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuUtbqPFtnyZ"
      },
      "source": [
        "6. Load your ML model, and use the model and Spark to perform the following:  \n",
        "    a) Every 10 seconds, print the total number of applications and number of potential default applications (prediction = 1) in the last 1 minute.  \n",
        "    b) Every 15 seconds, show the total requested credit (sum of credit where default=0) in the last 15 seconds.  \n",
        "    c) Every 1 minute, show the top 10 largest loan applications with the potential of default.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yreOFPmtnyZ"
      },
      "outputs": [],
      "source": [
        "## Load ML model\n",
        "\n",
        "model_path = 'loan_default_prediction_model'\n",
        "gbtModel = GBTClassificationModel.load(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEH5cvLxtnya"
      },
      "source": [
        "**Explanation**\n",
        "\n",
        "When completing assignment 2A, I forgot to save the transformation part of the model, only saved the gradient boosting tree model. I am unable to resolve this problem. Therefore, I will proceed with the rest of this assignment using target column in application_data_stream as the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65KMObZttnya"
      },
      "outputs": [],
      "source": [
        "# 6a\n",
        "application_breakdown = (df_final\n",
        "                         .groupBy(window(df_application.ts, \"1 minute\", \"10 seconds\"))\n",
        "                         .agg(F.count(\"target\").alias(\"total_number_of_applications\"),\n",
        "                              F.count(F.when(F.col(\"target\") == 1, True)).alias(\"number_of_potential_default_applications\"))\n",
        "                         .select(\"window\", \"total_number_of_applications\", \"number_of_potential_default_applications\")\n",
        "                        )\n",
        "\n",
        "#SEND OUTPUT TO CONSOLE SINK\n",
        "query1 = (application_breakdown\n",
        "          .orderBy(col(\"window\").start)\n",
        "          .writeStream.outputMode(\"complete\")\n",
        "          .format(\"console\")\n",
        "          .trigger(processingTime='10 seconds')\n",
        "          .option(\"truncate\",\"false\")\n",
        "          .start()\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22heAL54tnya"
      },
      "outputs": [],
      "source": [
        "query1.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nNht-SRtnyb"
      },
      "outputs": [],
      "source": [
        "# 6b\n",
        "total_credit_request = (df_final\n",
        "                        .groupBy(window(df_application.ts, \"15 seconds\", \"15 seconds\"))\n",
        "                        .agg(F.sum(F.when(F.col(\"target\") == 0, F.col(\"amt_credit\")).otherwise(0)).alias(\"total_credit_request\"))\n",
        "                        .select(\"window\", \"total_credit_request\")\n",
        "                       )\n",
        "\n",
        "#SEND OUTPUT TO CONSOLE SINK\n",
        "query2 = (total_credit_request\n",
        "          .orderBy(col(\"window\").start)\n",
        "          .writeStream.outputMode(\"complete\")\n",
        "          .format(\"console\")\n",
        "          .trigger(processingTime='15 seconds')\n",
        "          .option(\"truncate\",\"false\")\n",
        "          .start()\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_qF5cvetnyb"
      },
      "outputs": [],
      "source": [
        "query2.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veejWcpktnyb"
      },
      "outputs": [],
      "source": [
        "# 6c\n",
        "def process_batch(df, epoch_id):\n",
        "\n",
        "    top_10_potential_default = (df\n",
        "                                .filter(F.col(\"target\") == 1)\n",
        "                                .orderBy(F.col(\"amt_credit\").desc())\n",
        "                                .limit(10)\n",
        "                                .select(\"id_app\",\"amt_credit\",\"ts\")\n",
        "                               )\n",
        "\n",
        "    top_10_potential_default.show()\n",
        "\n",
        "query3 = (df_final\n",
        "          .writeStream\n",
        "          .format(\"console\")\n",
        "          .foreachBatch(process_batch)\n",
        "          .trigger(processingTime='1 minute')\n",
        "          .option(\"truncate\",\"false\")\n",
        "          .start()\n",
        "         )\n",
        "\n",
        "## Reference: (n.d.). How to use foreach or foreachBatch in PySpark to write to database? Stack Overflow. https://stackoverflow.com/questions/58766638/how-to-use-foreach-or-foreachbatch-in-pyspark-to-write-to-database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyHTjcIetnyc",
        "outputId": "082cac96-a6ff-4716-ec4d-fb8c24e6b778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+---+\n",
            "|id_app|amt_credit| ts|\n",
            "+------+----------+---+\n",
            "+------+----------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query3.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmHrBKYjtnyc"
      },
      "source": [
        "7. Write a Parquet file to store the following data:  \n",
        "    a) Persist the raw data: Persist your prediction results along with application data and event_time in Parquet format; after that, read the Parquet file and show the first 10 records.  \n",
        "    b) Persist the 15-second requested credit (6b) in another parquet file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dG70FUUtnyd"
      },
      "outputs": [],
      "source": [
        "# 7a\n",
        "# Write into parquet files with the prediction, application data and event time\n",
        "query_file_sink_a = (df_final\n",
        "                    .writeStream\n",
        "                    .format(\"parquet\")\n",
        "                    .outputMode(\"append\")\n",
        "                    .option(\"path\", \"parquet/application_df\")\n",
        "                    .option(\"checkpointLocation\", \"parquet/application_df/checkpoint\")\n",
        "                    .start()\n",
        "                   )\n",
        "\n",
        "## Reference: (2024, January 29). FIT5202 - Week11 - 1. Spark Streaming Watermarking DEMO [Lab Resources]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdikuMhZtnyd"
      },
      "outputs": [],
      "source": [
        "#Stop the file_sink query\n",
        "query_file_sink_a.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlayC2AOtnye",
        "outputId": "808448ab-071d-4084-de5c-b25404a34102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-------------+------+-------+------------+---------------+------------+----------+-----------+---------------+-----------+--------------+-------------+------------+-----------------+----------+-------------+-----------+-----------+--------------+---------------+----------------+----------+----------+---------------+---------------+-------------------------+----------------------+-----------------+--------------+--------------+--------------+----------------------+------------------------+-----------------------+------------------------+-------------------------+---------------------------+------------------------+-------------------+\n",
            "|id_app|target|contract_type|gender|own_car|own_property|num_of_children|income_total|amt_credit|amt_annuity|amt_goods_price|income_type|education_type|family_status|housing_type|region_population|days_birth|days_employed|own_car_age|flag_mobile|flag_emp_phone|flag_work_phone|flag_cont_mobile|flag_phone|flag_email|occupation_type|cnt_fam_members|weekday_app_process_start|hour_app_process_start|organization_type|credit_score_1|credit_score_2|credit_score_3|days_last_phone_change|amt_credit_req_last_hour|amt_credit_req_last_day|amt_credit_req_last_week|amt_credit_req_last_month|amt_credit_req_last_quarter|amt_credit_req_last_year|                 ts|\n",
            "+------+------+-------------+------+-------+------------+---------------+------------+----------+-----------+---------------+-----------+--------------+-------------+------------+-----------------+----------+-------------+-----------+-----------+--------------+---------------+----------------+----------+----------+---------------+---------------+-------------------------+----------------------+-----------------+--------------+--------------+--------------+----------------------+------------------------+-----------------------+------------------------+-------------------------+---------------------------+------------------------+-------------------+\n",
            "|393219|     0|            2|     F|      Y|           Y|              0|    166500.0|  755190.0|    33394.5|       675000.0|          5|             1|            3|           6|         0.025164|    -21549|       365243|       19.0|          1|             0|              0|               1|         1|         0|             18|            2.0|                  TUESDAY|                     7|               31|          NULL|    0.64951134|    0.64136827|                 -1580|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     6.0|2024-02-15 18:06:56|\n",
            "|393220|     0|            2|     F|      N|           N|              0|     67500.0|  727785.0|    26734.5|       607500.0|          2|             1|            3|           6|         0.031329|    -10526|        -1004|       NULL|          1|             1|              0|               1|         1|         0|             16|            2.0|                 THURSDAY|                    12|               36|          NULL|     0.6318137|     0.6817059|                 -2123|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     2.0|2024-02-15 18:06:56|\n",
            "|393299|     0|            2|     F|      Y|           Y|              0|    135000.0|  170640.0|     8428.5|       135000.0|          2|             4|            2|           6|         0.035792|    -11940|         -939|       10.0|          1|             1|              0|               1|         0|         0|             19|            2.0|                   FRIDAY|                    13|               50|          NULL|     0.5790141|          NULL|                  -607|                     0.0|                    0.0|                     0.0|                      0.0|                        2.0|                     1.0|2024-02-15 18:06:56|\n",
            "|393221|     0|            2|     M|      N|           N|              0|    171000.0|  472500.0|    18436.5|       472500.0|          8|             2|            6|           6|          0.04622|     -8321|         -185|       NULL|          1|             1|              1|               1|         0|         0|             19|            1.0|                   SUNDAY|                    11|               21|   0.037742544|     0.6729242|    0.11911906|                  -718|                     0.0|                    0.0|                     0.0|                      1.0|                        0.0|                     2.0|2024-02-15 18:06:56|\n",
            "|393222|     0|            2|     F|      N|           Y|              0|    135000.0|  269550.0|    19300.5|       225000.0|          5|             1|            3|           6|         0.028663|    -24073|       365243|       NULL|          1|             0|              0|               1|         0|         0|             18|            2.0|                   FRIDAY|                    14|               31|          NULL|    0.70294255|          NULL|                  -105|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     5.0|2024-02-15 18:06:56|\n",
            "|393223|     0|            2|     F|      N|           Y|              0|    180000.0|  432841.5|    42295.5|       391500.0|          5|             1|            5|           6|         0.020713|    -23184|       365243|       NULL|          1|             0|              0|               1|         0|         0|             18|            1.0|                   FRIDAY|                     7|               31|          NULL|    0.37780404|    0.56560796|                 -2382|                     0.0|                    0.0|                     0.0|                      1.0|                        0.0|                     4.0|2024-02-15 18:06:56|\n",
            "|393225|     0|            2|     F|      N|           N|              0|    166500.0|  956574.0|    48969.0|       855000.0|          8|             4|            6|           6|         0.035792|     -9249|        -1057|       NULL|          1|             1|              1|               1|         1|         0|             12|            1.0|                WEDNESDAY|                    17|               36|    0.35150006|    0.62137157|          NULL|                 -1385|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     4.0|2024-02-15 18:06:56|\n",
            "|393226|     0|            1|     M|      N|           N|              2|    180000.0|  270000.0|    13500.0|       270000.0|          2|             4|            6|           6|         0.007114|    -15696|         -652|       NULL|          1|             1|              0|               1|         0|         0|             12|            3.0|                 THURSDAY|                    18|               50|          NULL|    0.40834218|     0.5513813|                  -866|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     1.0|2024-02-15 18:06:56|\n",
            "|393228|     0|            1|     F|      Y|           Y|              1|    126000.0|  247500.0|    12375.0|       247500.0|          2|             1|            3|           6|         0.010147|    -10940|        -1783|        9.0|          1|             1|              0|               1|         0|         0|              7|            3.0|                  TUESDAY|                    15|               43|          NULL|   0.025046552|    0.19747451|                 -1280|                     0.0|                    0.0|                     0.0|                      0.0|                        0.0|                     2.0|2024-02-15 18:06:56|\n",
            "|393229|     0|            2|     F|      N|           Y|              2|    157500.0| 1971072.0|    68643.0|      1800000.0|          2|             1|            3|           6|         0.020713|    -12949|        -5217|       NULL|          1|             1|              0|               1|         0|         0|             11|            4.0|                WEDNESDAY|                    12|               22|     0.5744009|     0.5696266|          NULL|                  -857|                    NULL|                   NULL|                    NULL|                     NULL|                       NULL|                    NULL|2024-02-15 18:06:56|\n",
            "+------+------+-------------+------+-------+------------+---------------+------------+----------+-----------+---------------+-----------+--------------+-------------+------------+-----------------+----------+-------------+-----------+-----------+--------------+---------------+----------------+----------+----------+---------------+---------------+-------------------------+----------------------+-----------------+--------------+--------------+--------------+----------------------+------------------------+-----------------------+------------------------+-------------------------+---------------------------+------------------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the saved parquet data\n",
        "query_file_sink_a_df = spark.read.parquet(\"parquet/application_df\")\n",
        "query_file_sink_a_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lFI-yf8tnyf"
      },
      "outputs": [],
      "source": [
        "# 7b\n",
        "# Write into parquet files with requested credit\n",
        "query_file_sink_b = (total_credit_request\n",
        "                     .writeStream.format(\"parquet\")\n",
        "                     .outputMode(\"append\")\n",
        "                     .option(\"path\", \"parquet/total_credit_request\")\n",
        "                     .option(\"checkpointLocation\", \"parquet/total_credit_request/checkpoint\")\n",
        "                     .start()\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apVJnvAWtnyg",
        "outputId": "9321d17b-0c45-4413-d59e-2636e78dc1fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|              window|total_credit_request|\n",
            "+--------------------+--------------------+\n",
            "|{2024-02-15 16:51...|       7.342874685E8|\n",
            "|{2024-02-15 16:51...|       4.158635895E8|\n",
            "|{2024-02-15 16:52...|       4.926838995E8|\n",
            "|{2024-02-15 17:10...|        4.62045924E8|\n",
            "|{2024-02-15 17:16...|        5.34052908E8|\n",
            "|{2024-02-15 17:10...|       4.448438235E8|\n",
            "|{2024-02-15 16:53...|       4.791040245E8|\n",
            "|{2024-02-15 17:15...|        2.05130475E8|\n",
            "|{2024-02-15 16:53...|        3.48389037E8|\n",
            "|{2024-02-15 16:52...|       7.349625315E8|\n",
            "+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the saved parquet data\n",
        "query_file_sink_b_df = spark.read.parquet(\"parquet/total_credit_request\")\n",
        "\n",
        "query_file_sink_b_df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU8XJcXOtnyg"
      },
      "source": [
        "8. Read the two parquet files from 7a and 7b as a data stream, and send the records to two topics with appropriate names.  \n",
        "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet files. The parquet files serve as an intermediate storage in this use case.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbUyhUxItnyh"
      },
      "outputs": [],
      "source": [
        "## Keep updating and save parquet files from 7a and 7b\n",
        "\n",
        "# The content for parquet file from 7b is already saved at \"parquet/total_credit_request\"\n",
        "\n",
        "# Write into parquet files with requested credit\n",
        "query_file_sink_c = (application_breakdown\n",
        "                     .writeStream.format(\"parquet\")\n",
        "                     .outputMode(\"append\")\n",
        "                     .option(\"path\", \"parquet/application_breakdown\")\n",
        "                     .option(\"checkpointLocation\", \"parquet/application_breakdown/checkpoint\")\n",
        "                     .start()\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iC-R92gtnyh",
        "outputId": "6790ccee-c3b4-44ae-c404-5b07abe5f5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------------------------+----------------------------------------+\n",
            "|              window|total_number_of_applications|number_of_potential_default_applications|\n",
            "+--------------------+----------------------------+----------------------------------------+\n",
            "|{2024-02-15 17:15...|                        3417|                                     289|\n",
            "|{2024-02-15 17:09...|                        1553|                                     145|\n",
            "|{2024-02-15 17:11...|                        3570|                                     289|\n",
            "|{2024-02-15 17:14...|                         649|                                      59|\n",
            "|{2024-02-15 17:11...|                        3744|                                     299|\n",
            "|{2024-02-15 17:14...|                         150|                                      12|\n",
            "|{2024-02-15 17:10...|                        3236|                                     277|\n",
            "|{2024-02-15 17:11...|                        3050|                                     238|\n",
            "|{2024-02-15 17:16...|                        3604|                                     282|\n",
            "|{2024-02-15 17:10...|                        3143|                                     259|\n",
            "+--------------------+----------------------------+----------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the saved parquet data to inspect the records\n",
        "query_file_sink_c_df = spark.read.parquet(\"parquet/application_breakdown\")\n",
        "\n",
        "query_file_sink_c_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu_eWqzrtnyu"
      },
      "outputs": [],
      "source": [
        "# Stream 1\n",
        "\n",
        "# Define the schema for the dataframe\n",
        "schema1 = StructType([\n",
        "    StructField(\"window\", StructType([StructField(\"start\", TimestampType(), True),StructField(\"end\", TimestampType(), True)]), True),\n",
        "    StructField(\"total_number_of_applications\", LongType(), True),\n",
        "    StructField(\"number_of_potential_default_applications\", LongType(), True),\n",
        "])\n",
        "\n",
        "# Read the streamed dataframe\n",
        "stream_application_breakdown = spark.readStream.format(\"parquet\").schema(schema1).load(\"parquet/application_breakdown\")\n",
        "\n",
        "\n",
        "# Write the streamed dataframe to kafka\n",
        "query_a = (stream_application_breakdown\n",
        "           .selectExpr(\"CAST(null AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
        "           .writeStream\n",
        "           .outputMode(\"append\")\n",
        "           .format(\"kafka\")\n",
        "           .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
        "           .option(\"topic\", \"application_visulisation_1\")\n",
        "           .option(\"checkpointLocation\", \"checkpoint/application_breakdown\")\n",
        "           .start()\n",
        "          )\n",
        "\n",
        "## Reference: (2024, January 29). FIT5202 - Week11 - 1. Spark Streaming Watermarking DEMO [Lab Resources].\n",
        "## Reference: (n.d.). PySpark : Write Spark Dataframe to Kafka Topic. Stack Overflow. https://stackoverflow.com/questions/62370231/pyspark-write-spark-dataframe-to-kafka-topic\n",
        "## Reference: (n.d.). Spark: How to define a nested schema? Stack Overflow. https://stackoverflow.com/questions/67379564/spark-how-to-define-a-nested-schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPpsaVeKtnyu"
      },
      "outputs": [],
      "source": [
        "# Stream 2\n",
        "\n",
        "# Define the schema for the dataframe\n",
        "schema2 = StructType([\n",
        "    StructField(\"window\", StructType([StructField(\"start\", TimestampType(), True),StructField(\"end\", TimestampType(), True)]), True),\n",
        "    StructField(\"total_credit_request\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "# Read the streamed dataframe\n",
        "stream_total_credit_request = spark.readStream.format(\"parquet\").schema(schema2).load(\"parquet/total_credit_request\")\n",
        "\n",
        "# Write the streamed dataframe to kafka\n",
        "query_b = (stream_total_credit_request\n",
        "           .selectExpr(\"CAST(null AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
        "           .writeStream\n",
        "           .outputMode(\"append\")\n",
        "           .format(\"kafka\")\n",
        "           .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
        "           .option(\"topic\", \"application_visulisation_2\")\n",
        "           .option(\"checkpointLocation\", \"checkpoint/total_credit_request\")\n",
        "           .start()\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUIBBmmRtnyv",
        "outputId": "ddf4cbdd-21a0-44ad-b2cd-5331467022b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|              window|total_credit_request|\n",
            "+--------------------+--------------------+\n",
            "|{2024-02-16 16:34...|         3.6846594E7|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Use the following code to validate streamed data\n",
        "\n",
        "# df_test = spark.read.parquet('parquet/total_credit_request/part-00022-6e089cd7-d523-4bef-91ee-864ddfa674aa-c000.snappy.parquet')\n",
        "# df_test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBn4-Comtnyv",
        "outputId": "6f329392-3cc2-4631-e3c6-eb6f5187bbe7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Use the following code to inspect streaming status\n",
        "\n",
        "query_b.status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruO44Mwltnyw"
      },
      "outputs": [],
      "source": [
        "## Stop the queries\n",
        "query_file_sink_b.stop()\n",
        "query_file_sink_c.stop()\n",
        "\n",
        "query_a.stop()\n",
        "query_b.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}